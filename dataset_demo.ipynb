{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dce81f-e020-4a91-89f1-13eea51c9c11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5525b3-9c87-4183-b8ba-038b4775415e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tempfile\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "from vicas.dataset import ViCaSDataset, ViCaSVideo\n",
    "from vicas.caption_parsing import parse_caption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da753015-51a6-4171-b1ad-d725e967aede",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset API\n",
    "\n",
    "`ViCaSDataset` is a wrapper class to easily iterate over all videos.\n",
    "\n",
    "**TODO:** Set `annotations_dir` to the directory path where all the JSON annotations are saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f863268-a2a6-4503-92cc-7496b0c32c3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "annotations_dir = \"/path/to/vicas/json/annotations/dir\"\n",
    "video_frames_dir = \"demo_data/video_frames\"\n",
    "split = None # can be set to 'train', 'val' or 'test' to load a particular split\n",
    "\n",
    "dataset = ViCaSDataset(\n",
    "    annotations_dir, \n",
    "    split=split,\n",
    "    video_frames_dir=video_frames_dir\n",
    ")\n",
    "print(f\"Indexed {len(dataset)} videos from the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d30b765-ff6f-4051-b66d-c96091a76ac9",
   "metadata": {},
   "source": [
    "## Video API\n",
    "\n",
    "`ViCaSVideo` is a wrapper for each video. You can instantiate it through the dataset, or also separately by running `ViCaSVideo.from_json(...)` and providing the path to the JSON file for that video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77b9121-b762-4982-a76f-a7b316dbbe4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example_video_id = 9505  # videos for a few example videos are provided in 'demo_data'.\n",
    "video = dataset.parse_video(example_video_id)\n",
    "# Alternate:\n",
    "# ViCaSVideo.from_json(f\"{annotations_dir}/{example_video_id:06d}.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7331fff-c45a-40ac-9530-4f79c4d501ca",
   "metadata": {},
   "source": [
    "#### Visualization\n",
    "\n",
    "After running `video.visualize()`, two outputs will be saved to disk:\n",
    "- The visualization will be saved as a video to `<viz_temp_dir>/video.mp4`\n",
    "- The individual video frames of the visualization will be saved to a sub-directory at `<viz_temp_dir/frames`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97abfa04-d922-4cd7-80d0-88fa6e840a42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "viz_temp_dir = os.path.join(tempfile.gettempdir(), \"ViCaS_demo\", f\"{example_video_id:06d}\")\n",
    "os.makedirs(viz_temp_dir, exist_ok=True)\n",
    "print(f\"Visualization output will be saved to: {viz_temp_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95211854-732d-4f30-8fa7-4627b9361ffd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "video.visualize(viz_temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27b5fbe-51ac-49b7-9f62-f48f38b63606",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# helper function to play video\n",
    "def play_video(filename):\n",
    "    html = ''\n",
    "    with open(filename, 'rb') as fh:\n",
    "        video = fh.read()\n",
    "    src = 'data:video/mp4;base64,' + b64encode(video).decode()\n",
    "    html += '<video width=600 controls autoplay loop><source src=\"%s\" type=\"video/mp4\"></video>' % src \n",
    "    return HTML(html)\n",
    "\n",
    "# play the visualization video\n",
    "play_video(os.path.join(viz_temp_dir, \"video.mp4\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6011712-423c-4d53-aa0b-9a096fdd70e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Captions\n",
    "\n",
    "The `ViCaSVideo` object contains multiple properties for the caption:\n",
    "- `caption_orig_raw`: This is the original, human-written caption with our custom syntax for marking phrase grounding\n",
    "- `caption_orig_parsed`: This is the same as above, but with the custom syntax stripped away i.e. a standard caption\n",
    "- `caption_gpt_raw`: The result of using GPT4 to remove errors and improve the wording of `caption_orig_raw`.\n",
    "- `caption_gpt_parsed`: This is the same as above, but with the custom syntax stripped away.\n",
    "\n",
    "**NOTE:** We use the GPT-improved captions are used for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f091d8b5-0aa4-4fd9-a899-23bc709201ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Caption with phrase-grounding syntax: \" + video.caption_gpt_raw)\n",
    "print(\"Parsed caption without grounding syntax: \" + video.caption_gpt_parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3578702d-cc7d-436b-a3f6-a43278bee31c",
   "metadata": {},
   "source": [
    "**NOTE:** We also provide an API to parse the raw caption with syntax if you want to programmatically extract the phrase-grounding information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a76f18-7641-435a-8b4e-856c7f3a2ed0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "caption_obj = parse_caption(video.caption_gpt_raw)\n",
    "print(\"Parsed caption without grounding syntax: \" + caption_obj.parsed)  # same as the parsed caption printed above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec05f86-35ff-43ae-9969-588b2917c8ca",
   "metadata": {},
   "source": [
    "You can call print on a `VideoCaption` to pretty-print all the attributes of the caption: the raw and parsed version, and a list of grounding phrases including the object IDs, the string indices of the phrase, and the phrase itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b26a895-b276-4673-aa72-31c85c09d1ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(caption_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a468062-7c6c-4e43-9a17-ece300ec2b5c",
   "metadata": {},
   "source": [
    "#### Language-Guided Video Instance Segmentation (LG-VIS)\n",
    "\n",
    "The LG-VIS prompts and associated masks can be obtained by calling `video.parse_lgvis()`. This function returns an iterator over the prompts. Each element is a tuple with four entires:\n",
    "- **prompt** *(str)*: The text prompt\n",
    "- **masks** *(List[List[np.ndarray]])*: The object masks. The inner list is over different objects (a single prompt can reference multiple objects). The outer-list is over time/frames.\n",
    "- **track_ids** *(List[int])*: The IDs of the objects.\n",
    "- **filenames** *(List[str])*: The filenames of the video frames (same length as the `masks`).\n",
    "- Optional: **viz_frames** *(List[np.ndarray])*: If `return_viz` is set to true, a list of image frames with the prompt and mask visualized will be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fb016b-7e6e-4dd3-9cd7-32aeec3ba998",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"This video contains {video.num_lgvis_prompts} LG-VIS prompts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607afede-8d20-4002-84bf-c3c488095fa7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for prompt, masks, track_ids, filenames, viz_frames in video.parse_lgvis(return_viz=True): # iterate over prompts\n",
    "    print(\"Prompt: \" + prompt)\n",
    "    print(f\"There are {len(masks)} frame-level masks\")\n",
    "    print(f\"This prompt references {len(masks[0])} object tracks with IDs {track_ids}\")\n",
    "    print(f\"Each mask array has shape {masks[0][0].shape} and dtype {masks[0][0].dtype}\")\n",
    "    \n",
    "    # display 6 frames\n",
    "    frame_indices = np.linspace(0, len(viz_frames)-1, 6).astype(int).tolist()\n",
    "    viz_frames = [viz_frames[i] for i in frame_indices]\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(viz_frames[i][:, :, ::-1]) # convert image BGR to RGB\n",
    "        ax.axis('off')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
