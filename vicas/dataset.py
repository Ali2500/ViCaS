from typing import Optional, List, Tuple, Dict, Any, Union
from glob import glob

import copy
import multiprocessing as mp
import numpy as np
import os.path as osp
import pycocotools.mask as mt
import json

from vicas.caption_parsing import parse_caption, VideoCaption, VideoCaptionPhrase
from vicas.viz_utils import generate_video_viz, visualize_lgvis_prompts


class ViCaSVideo:
    def __init__(self, json_content: Dict[str, Any], frames_dir: Optional[str] = None):
        # Meta info
        self.video_id            = json_content['video_id']
        self.src_dataset         = json_content['src_dataset']
        self.filename            = json_content['filename']
        self.image_size          = json_content['image_size']

        # Captions
        self.caption_orig_raw    = json_content['caption_raw_en']
        self.caption_orig_parsed = json_content['caption_parsed_en']
        self.caption_gpt_raw     = json_content['caption_raw_en_gpt']
        self.caption_gpt_parsed  = json_content['caption_parsed_en_gpt']
        self.caption_chinese_raw = json_content['caption_raw_cn']
        self.captions_reworded   = json_content['reworded_en_captions']

        # Referral Expressions
        self.lgvis_exprs         = json_content['object_referrals']
        
        # Segmentation Masks
        self._segmentations      = json_content['segmentations']
        self.track_ids           = json_content['all_track_ids']

        self.frames_dir = frames_dir

    @property
    def num_lgvis_prompts(self):
        return len(self.lgvis_exprs)

    def parsed_caption_object_raw(self) -> VideoCaption:
        return parse_caption(self.caption_orig_raw)

    def parsed_caption_object_gpt(self) -> VideoCaption:
        return parse_caption(self.caption_gpt_raw)

    def visualize(self, output_dir, framerate: Optional[int] = 15):
        assert self.frames_dir is not None, f"Frames directory must be set for visualization"
        output_frames = osp.join(output_dir, "frames")
        output_video = osp.join(output_dir, "video.mp4")
        num_workers = min(8, mp.cpu_count())

        generate_video_viz(
            caption_raw=self.caption_gpt_raw,
            json_content_segmentations=self._segmentations,
            video_frames_dir=self.frames_dir,
            image_size=self.image_size,
            labeled_frames_output_dir=output_frames,
            labeled_video_output_path=output_video,
            num_workers=num_workers,
            framerate=framerate
        )

    def parse_lgvis(self, include_auto_annotated_masks: bool = True, return_viz: bool = False):
        """
        Return LG-VIS prompts and corresponding segmentation masks 

        Args:
            include_auto_annotated_masks (bool): If true, masks generated by SAM2 at 30fps will be returned. If false, only the 
            human-annotated masks at 1fps will be returned.
            return_viz (bool). If true, a list of video frames will be returned with the visualization of the prompt

        Returns:
            Iterator object. Each element is a tuple with three elements:
                - prompt: the language prompt
                - masks: A nested list where the outer list refers to each frame and the inner frame is over the objects 
                         referenced by the prompt
                - track_ids: The track IDs for the referenced objects
                - filenames: Filenames of the image frames (same length as `masks`)
        """
        for expr in self.lgvis_exprs:
            prompt = expr['prompt']
            track_ids = expr['track_ids']
            filenames, masks = self.segmentations(track_ids, masks_as_array=True, include_auto_annotated_masks=include_auto_annotated_masks)
            masks = [
                [masks_t.get(t_id, self.get_null_mask()) for t_id in track_ids] for masks_t in masks
            ]

            if return_viz:
                viz_frames = visualize_lgvis_prompts(prompt, masks, filenames, osp.dirname(self.frames_dir), font_scale=1.5, font_thickness=2, show_progress_bar=True)
                yield prompt, masks, track_ids, filenames, viz_frames
            else:
                yield prompt, masks, track_ids, filenames

    def segmentations(
        self, 
        track_ids: Optional[List[int]] = None,
        masks_as_array: Optional[bool] = True, 
        include_auto_annotated_masks: Optional[bool] = True
    ) -> Tuple[List[str], List[Dict[str, Any]]]:
        """
        Parse segmentation masks.

        Args:
            track_ids: List of track IDs to parse. If not given, masks for all tracks will be returned.
            masks_as_array (bool): If true, the masks as numpy arrays. If false, the RLE-encoded string will be returned.
            include_auto_annotated_masks (bool): If true, masks generated by SAM2 at 30fps will be returned. If false, only the 
            human-annotated masks at 1fps will be returned.

        Returns:
            A tuple of lists (filenames, masks). `filenames` contains the name of the image frame file. `masks` is a list of dicts. 
            They keys of each dict are object/track IDs and the values are masks (either as RLE-encoded strings or NumPy arrays). 
        """
        seg_list = []
        frame_filenames = []

        if not track_ids:
            track_ids = self.track_ids.copy()

        for segs_t in self._segmentations:
            if not segs_t["is_gt"] and not include_auto_annotated_masks:
                continue

            frame_filenames.append(osp.join(f"{self.video_id:06d}", segs_t['filename']))
            segs_t_dict = {}
            for track_id, mask_rle in zip(segs_t["track_ids"], segs_t["mask_rles"]):
                if track_id not in track_ids:
                    continue

                mask_rle = copy.deepcopy(mask_rle)
                mask_rle["counts"] = mask_rle["counts"].encode('utf-8')
                if masks_as_array:
                    segs_t_dict[track_id] = mt.decode(mask_rle).astype(np.uint8)
                else:
                    segs_t_dict[track_id] = mask_rle

            seg_list.append(segs_t_dict)

        return frame_filenames, seg_list

    def get_null_mask(self):
        return np.zeros(self.image_size, np.uint8)

    @classmethod
    def from_json(cls, json_path: str):
        with open(json_path, 'r') as fh:
            json_content = json.load(fh)

        return cls(json_content)


class ViCaSDataset:
    def __init__(
        self, 
        annotations_dir: str, 
        split: Optional[str] = None,
        video_dir: Optional[str] = None, 
        video_frames_dir: Optional[str] = None
    ):
        json_paths = sorted(glob(osp.join(annotations_dir, "*.json")))
        assert json_paths, f"No JSON files found in annotations_dir at '{annotations_dir}'"

        self.video_id_to_json = {
            int(osp.split(p)[-1].replace(".json", "")): p
            for p in json_paths
        }

        if split is not None:
            split_video_ids = self.get_split_videos(split)
            self.video_id_to_json = {
                video_id: json_path for video_id, json_path in self.video_id_to_json.items() if video_id in split_video_ids
            }
            assert len(self.video_id_to_json) == len(split_video_ids)

        self.video_dir = video_dir
        self.video_frames_dir = video_frames_dir

    def __len__(self):
        return len(self.video_id_to_json)

    def video_ids(self):
        return sorted(list(self.video_id_to_json.keys()))

    def parse_video(self, video_id: int):
        if video_id not in self.video_id_to_json:
            raise ValueError(f"No annotation file exists for video ID {video_id}")

        with open(self.video_id_to_json[video_id], 'r') as fh:
            content = json.load(fh)

        if self.video_frames_dir is None:
            return ViCaSVideo(content)
        else:
            frames_dir = osp.join(self.video_frames_dir, f"{video_id:06d}")
            return ViCaSVideo(content, frames_dir=frames_dir)

    def iter_videos(self):
        for video_id in self.video_id_to_json.keys():
            yield self.parse_video(video_id)

    def get_split_videos(self, split: str, version: Optional[str] = "v0.1"):
        assert split in ("train", "val", "test")
        expected_path = osp.join(osp.dirname(__file__), osp.pardir, "splits", version, f"{split}.json")
        assert osp.exists(expected_path), f"Split file not found at {expected_path}"
        with open(expected_path, 'r') as fh:
            return json.load(fh)
